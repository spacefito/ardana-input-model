#
# (c) Copyright 2016 Hewlett Packard Enterprise Development LP
# (c) Copyright 2017-2018 SUSE LLC
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
---
product:
  version: 2
pass-through:
  global:
    vmware:
      - username: <vcenter-admin-username>
        ip: <vcenter-ip>
        port: 443
        cert_check: false
        # The password needs to be encrypted using the script
        # openstack/ardana/ansible/ardanaencrypt.py on the deployer:
        #
        # $ cd ~/openstack/ardana/ansible
        # $ export ARDANA_USER_PASSWORD_ENCRYPT_KEY=<encryption key>
        # $ ./ardanaencrypt.py
        #
        # The script will prompt for 'unencrypted value?'
        # Enter the vCenter password in clear string.
        # The string generated will be the encrypted password.
        # Enter the string enclosed by double-quotes below.
        password: "<encrypted-passwd-from-ardanaencrypt>"

        # The id is obtained from the URL
        # https://<vcenter-ip>/mob/?moid=ServiceInstance&doPath=content.about
        # Select the field instanceUUID and copy paste the 'value' of the
        # field instanceUUID below.
        id: <vcenter-uuid>
        # The location of the resource configuration file
        vc_net_resources: <resource-configuration-file>



    # The below 'servers' section is used to define variables for
    # the various esx-compute-proxy and esx-ovsvapp appliances.
    #
    # The 'id' refers to the name of the node (ESX hypervisor) running the
    # esx-compute-proxy. This is identical to the 'servers.id' in
    # servers.yml. There should be one esx-compute-proxy node per ESX
    # resource pool or cluster.
    #
    # cluster_dvs_mapping should be of the format:
    # 'Datacenter-name/host/Cluster-Name:Trunk-DVS-Name',
    # where 'host' is the actual string 'host' and should not be changed or
    # substituted.
    #
    # vcenter_id is the same as the 'vcenter-uuid' obtained in the global
    # section.
    #
    # 'id': is the name of the appliance manually installed, 'ovsvapp1' for
    # example.
    #
    # 'vcenter_cluster': Name of the vcenter target cluster
    #
    # esx_hostname: hostname of the esx hypervisor hosting the ovsvapp
    # appliance
    #
    # NOTE: For every esx host in a cluster there should be an ovsvapp
    # instance running.

    # Once the esx-compute-proxy and  esx-ovsvapp appliances
    # have been deployed, and the appropriate DVS and DVPGS
    # have been created, add the information for exs-compute-proxy
    # and esx-ovsvapp below.  There should be one ovsvapp per
    # ESX hypervisor, and one esx-compute-proxy per Vmware cluster


# Only uncomment below this line
#  servers:
    #-
    #  id: esx-compute1
    #  data:
    #    vmware:
    #      vcenter_cluster: <vmware cluster1 name>
    #      vcenter_id: <vcenter-uuid>
    #-
    #  id: ovsvapp1
    #  data:
    #    vmware:
    #      vcenter_cluster: <vmware cluster1 name>
    #     cluster_dvs_mapping: <cluster dvs mapping>
    #      esx_hostname: <esx hostname hosting the ovsvapp>
    #      vcenter_id: <vcenter-uuid>
    #-
    #  id: ovsvapp2
    #  data:
    #    vmware:
    #      vcenter_cluster: <vmware cluster1 name>
    #      cluster_dvs_mapping: <cluster dvs mapping>
    #      esx_hostname: <esx hostname hosting the ovsvapp>
    #      vcenter_id: <vcenter-uuid>
